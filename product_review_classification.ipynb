{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as numpy\n",
    "import glob\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from nltk.collocations import BigramCollocationFinder, BigramAssocMeasures\n",
    "\n",
    "from itertools import islice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1122,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_root = './pos'\n",
    "neg_root = './neg'\n",
    "\n",
    "pos_files = glob.glob(pos_root+'/*.txt')\n",
    "neg_files = glob.glob(neg_root+'/*.txt')\n",
    "\n",
    "pos_corpus = []\n",
    "for pos_file in pos_files:\n",
    "    pos_corpus.append(open(pos_file).read())\n",
    "neg_corpus = []\n",
    "for neg_file in neg_files:\n",
    "    neg_corpus.append(open(neg_file).read())\n",
    "\n",
    "# remove unnecessary spaces\n",
    "for i in range(len(pos_corpus)):\n",
    "    pos_corpus[i] = TreebankWordDetokenizer().detokenize(word_tokenize(pos_corpus[i]))\n",
    "    #\" \".join(pos_corpus[i].split())\n",
    "for i in range(len(neg_corpus)):\n",
    "    neg_corpus[i] = TreebankWordDetokenizer().detokenize(word_tokenize(neg_corpus[i]))\n",
    "    #\" \".join(neg_corpus[i].split())\n",
    "    \n",
    "pos_data = pd.DataFrame(zip(pos_corpus, ['pos' for i in range(len(pos_corpus))]), columns=['text', 'sentiment'])\n",
    "neg_data = pd.DataFrame(zip(neg_corpus, ['neg' for i in range(len(neg_corpus))]), columns=['text', 'sentiment'])\n",
    "\n",
    "data = pd.concat([pos_data, neg_data])\n",
    "data = data.sample(frac=1).reset_index(drop=True) # shuffle the rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I have been trying to find a way to easily car...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Everything your read about the picture quality...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Researched a lot before I bought this TV and f...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Memorex DVD+R's are the best . Price keeps dro...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>If you have a cable modem for broadband connec...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text sentiment\n",
       "0  I have been trying to find a way to easily car...       pos\n",
       "1  Everything your read about the picture quality...       neg\n",
       "2  Researched a lot before I bought this TV and f...       pos\n",
       "3  Memorex DVD+R's are the best . Price keeps dro...       pos\n",
       "4  If you have a cable modem for broadband connec...       neg"
      ]
     },
     "execution_count": 1123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1124,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_lem = data['text'].copy()\n",
    "y_lem = data['sentiment'].copy()\n",
    "\n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "for i in range(len(X_lem)):\n",
    "    words = word_tokenize(X_lem[i])\n",
    "    pos_labels = pos_tag(words)\n",
    "    \n",
    "    for j in range(len(words)):\n",
    "        \n",
    "        pos_label = pos_labels[j][1][0].lower()\n",
    "#         pos_refs = {'n': ['NN', 'NNS', 'NNP', 'NNPS'],\n",
    "#                     'v': ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'],\n",
    "#                     'r': ['RB', 'RBR', 'RBS'],\n",
    "#                     'a': ['JJ', 'JJR', 'JJS']}\n",
    "\n",
    "        if pos_label == 'j':\n",
    "            pos_label = 'a'    # 'j' <--> 'a' reassignment for adjectives\n",
    "        \n",
    "        if pos_label in ['r']:  # for adverbs it's a bit different.\n",
    "            try:\n",
    "                # when a word doesn't have pertainym, use lemma\n",
    "                pertainym = wordnet.synset(words[j]+'.r.1').lemmas()[0].pertainyms()\n",
    "                if pertainym:\n",
    "                    words[j] = pertainym[0].name()\n",
    "                else:\n",
    "                    words[j] = wordnet.synset(words[j]+'.r.1').lemmas()[0].name()\n",
    "            except: # when a word doesn't have lemma, use original word\n",
    "                words[j] = words[j]\n",
    "        \n",
    "        elif pos_label in ['a', 's', 'v']: # for adjectives and verbs\n",
    "            words[j] = lem.lemmatize(words[j], pos=pos_label)\n",
    "        \n",
    "        else:   # for nouns and everything else as it is the default kwarg\n",
    "            words[j] = lem.lemmatize(words[j])\n",
    "            \n",
    "    X_lem[i] = TreebankWordDetokenizer().detokenize(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memorex DVD+R's are the best . Price keeps dropping and never have a problem\n",
      "---------------------------------------------------------------------\n",
      "Memorex DVD+R's be the best . Price keep drop and never have a problem\n"
     ]
    }
   ],
   "source": [
    "print(X[3])\n",
    "print('---------------------------------------------------------------------')\n",
    "print(X_lem[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply TF-IDF transformation with all combinations of lemmatization and stop words removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1125,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['text'].copy()\n",
    "y = data['sentiment'].copy()\n",
    "\n",
    "vectorizer = TfidfVectorizer(lowercase=True)\n",
    "vectorizer.fit(X)\n",
    "X_tfidf = vectorizer.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1126,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(lowercase=True)\n",
    "vectorizer.fit(X_lem)\n",
    "X_lem_tfidf = vectorizer.transform(X_lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1127,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_list = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \n",
    "                  \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", \n",
    "                  'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', \n",
    "                  'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', \n",
    "                  'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', \n",
    "                  'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', \n",
    "                  'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', \n",
    "                  'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', \n",
    "                  'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', \n",
    "                  'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'own', 'same', \n",
    "                  'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'should', \"should've\", 'now', 'd', \n",
    "                  'll', 'm', 'o', 're', 've', 'y', 'ma']\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=stopwords_list, lowercase=True)\n",
    "vectorizer.fit(X_lem)\n",
    "X_lem_sw_tfidf = vectorizer.transform(X_lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1128,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words=stopwords_list, lowercase=True)\n",
    "vectorizer.fit(X)\n",
    "X_sw_tfidf = vectorizer.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pointwise Mutual Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('customer_service', 14.187192118226601),\n",
       " ('caller_id', 11.834319526627219),\n",
       " ('escort_radar', 10.285714285714286),\n",
       " ('&_quot;', 8.937931034482759),\n",
       " ('tech_support', 8.28132727127702),\n",
       " ('does_not', 7.880766601696834),\n",
       " ('highly_recommend', 6.740720221606648),\n",
       " ('altec_lansing', 6.4935064935064934),\n",
       " ('sound_quality', 6.331670195763461),\n",
       " ('battery_life', 5.889407407407408),\n",
       " ('waste_money', 5.797083485235144),\n",
       " ('mp3_player', 5.5826083695815205),\n",
       " ('fiber_optic', 5.142857142857143),\n",
       " ('belt_clip', 4.71889400921659),\n",
       " ('do_not', 4.44265687870339),\n",
       " ('timely_manner', 4.166666666666667),\n",
       " ('bells_whistles', 4.0),\n",
       " ('mowing_lawn', 4.0),\n",
       " ('nvidia_quadrofx', 3.7925925925925927),\n",
       " ('ip_address', 3.6134868421052633),\n",
       " ('windows_xp', 3.2015289525048796),\n",
       " ('harman_kardon', 3.2),\n",
       " ('polycom_communicator', 3.125),\n",
       " ('ique_3600', 3.0625),\n",
       " ('(pci_express)', 3.0),\n",
       " ('horror_stories', 3.0),\n",
       " ('circuit_city', 2.9761904761904763),\n",
       " ('dvd_player', 2.860169491525424),\n",
       " ('nvidia_geforce', 2.8583333333333334),\n",
       " ('stopped_working', 2.8168379304128357)]"
      ]
     },
     "execution_count": 1129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words_sw = [word.lower()\n",
    "                for review in X\n",
    "                for word in review.replace('.', '').replace(',', '').split()\n",
    "                if not word.lower() in stopwords_list\n",
    "               ]\n",
    "\n",
    "finder = BigramCollocationFinder.from_words(all_words_sw)\n",
    "bgm = BigramAssocMeasures()\n",
    "score = bgm.mi_like  # metric options: pmi or mi_like\n",
    "collocations = {'_'.join(bigram): pmi for bigram, pmi in finder.score_ngrams(score)}\n",
    "\n",
    "list(islice(collocations.items(), 30)) # return word pairs with highest scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare models and fine tune the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_scores(scores):\n",
    "    k = len(scores['test_precision_macro'])\n",
    "    print('test_precision_macro:    ' + str(sum(scores['test_precision_macro']) / k))\n",
    "    print('test_recall_macro:       ' + str(sum(scores['test_recall_macro']) / k))\n",
    "    print('test_f1_macro:           ' + str(sum(scores['test_f1_macro']) / k))\n",
    "    print('test_precision_weighted: ' + str(sum(scores['test_precision_weighted']) / k))\n",
    "    print('test_recall_weighted:    ' + str(sum(scores['test_recall_weighted']) / k))\n",
    "    print('test_f1_weighted:        ' + str(sum(scores['test_f1_weighted']) / k))\n",
    "    \n",
    "scoring = ['precision_macro', 'recall_macro', 'f1_macro',\n",
    "           'precision_weighted', 'recall_weighted', 'f1_weighted']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### No lemmatization nor stop words removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB\n",
      "test_precision_macro:    0.8095240096178978\n",
      "test_recall_macro:       0.8074999999999999\n",
      "test_f1_macro:           0.8071715969042519\n",
      "test_precision_weighted: 0.8095240096178978\n",
      "test_recall_weighted:    0.8075000000000001\n",
      "test_f1_weighted:        0.8071715969042519\n",
      "-------------------------------------------\n",
      "LogisticRegression\n",
      "test_precision_macro:    0.8217443569197549\n",
      "test_recall_macro:       0.8215\n",
      "test_f1_macro:           0.8214654015692169\n",
      "test_precision_weighted: 0.8217443569197549\n",
      "test_recall_weighted:    0.8215000000000001\n",
      "test_f1_weighted:        0.8214654015692169\n",
      "-------------------------------------------\n",
      "SGDClassifier\n",
      "test_precision_macro:    0.8076322554685197\n",
      "test_recall_macro:       0.8065\n",
      "test_f1_macro:           0.8063215088534814\n",
      "test_precision_weighted: 0.8076322554685197\n",
      "test_recall_weighted:    0.8065\n",
      "test_f1_weighted:        0.8063215088534814\n",
      "-------------------------------------------\n",
      "RandomForestClassifier\n",
      "test_precision_macro:    0.8043527167312368\n",
      "test_recall_macro:       0.8039999999999999\n",
      "test_f1_macro:           0.8039430291132501\n",
      "test_precision_weighted: 0.8043527167312368\n",
      "test_recall_weighted:    0.8039999999999999\n",
      "test_f1_weighted:        0.8039430291132501\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('MultinomialNB')\n",
    "nb_model = MultinomialNB()\n",
    "\n",
    "scores = cross_validate(nb_model, X_tfidf, y, scoring=scoring, cv=5)\n",
    "print_scores(scores)\n",
    "print('-------------------------------------------')\n",
    "\n",
    "\n",
    "print('LogisticRegression')\n",
    "lr_model = LogisticRegression()\n",
    "\n",
    "scores = cross_validate(lr_model, X_tfidf, y, scoring=scoring, cv=5)\n",
    "print_scores(scores)\n",
    "print('-------------------------------------------')\n",
    "\n",
    "\n",
    "print('SGDClassifier')\n",
    "sgd_model = SGDClassifier()\n",
    "\n",
    "scores = cross_validate(sgd_model, X_tfidf, y, scoring=scoring, cv=5)\n",
    "print_scores(scores)\n",
    "print('-------------------------------------------')\n",
    "\n",
    "\n",
    "print('RandomForestClassifier')\n",
    "rf_model = RandomForestClassifier()\n",
    "\n",
    "scores = cross_validate(rf_model, X_tfidf, y, scoring=scoring, cv=5)\n",
    "print_scores(scores)\n",
    "print('-------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Only lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_precision_macro:    0.8079471286458528\n",
      "test_recall_macro:       0.8074999999999999\n",
      "test_f1_macro:           0.807434632573008\n",
      "test_precision_weighted: 0.8079471286458528\n",
      "test_recall_weighted:    0.8074999999999999\n",
      "test_f1_weighted:        0.807434632573008\n"
     ]
    }
   ],
   "source": [
    "lr_model = LogisticRegression()\n",
    "\n",
    "scores = cross_validate(lr_model, X_lem_tfidf, y, scoring=scoring, cv=5)\n",
    "print_scores(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatization + stop words removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_precision_macro:    0.8147274192737092\n",
      "test_recall_macro:       0.8140000000000001\n",
      "test_f1_macro:           0.8138971996152471\n",
      "test_precision_weighted: 0.814727419273709\n",
      "test_recall_weighted:    0.8140000000000001\n",
      "test_f1_weighted:        0.8138971996152471\n"
     ]
    }
   ],
   "source": [
    "lr_model = LogisticRegression()\n",
    "\n",
    "scores = cross_validate(lr_model, X_lem_sw_tfidf, y, scoring=scoring, cv=5)\n",
    "print_scores(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Only stop words removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_precision_macro:    0.8242027197314539\n",
      "test_recall_macro:       0.8234999999999999\n",
      "test_f1_macro:           0.8233981437841214\n",
      "test_precision_weighted: 0.8242027197314539\n",
      "test_recall_weighted:    0.8234999999999999\n",
      "test_f1_weighted:        0.8233981437841214\n"
     ]
    }
   ],
   "source": [
    "lr_model = LogisticRegression()\n",
    "\n",
    "scores = cross_validate(lr_model, X_sw_tfidf, y, scoring=scoring, cv=5)\n",
    "print_scores(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Only unigram + bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_precision_macro:    0.8323720189285286\n",
      "test_recall_macro:       0.8320000000000001\n",
      "test_f1_macro:           0.8319502431526324\n",
      "test_precision_weighted: 0.8323720189285286\n",
      "test_recall_weighted:    0.8320000000000001\n",
      "test_f1_weighted:        0.8319502431526324\n"
     ]
    }
   ],
   "source": [
    "vectorizer_ngram = TfidfVectorizer(lowercase=True, ngram_range=(1, 2), max_features=10000)\n",
    "vectorizer_ngram.fit(X)\n",
    "X_ngram_tfidf = vectorizer_ngram.transform(X)\n",
    "\n",
    "lr_model = LogisticRegression()\n",
    "\n",
    "scores = cross_validate(lr_model, X_ngram_tfidf, y, scoring=scoring, cv=5)\n",
    "print_scores(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_precision_macro:    0.8506426090164687\n",
      "test_recall_macro:       0.85\n",
      "test_f1_macro:           0.8499248086747737\n",
      "test_precision_weighted: 0.8506426090164687\n",
      "test_recall_weighted:    0.85\n",
      "test_f1_weighted:        0.8499248086747737\n"
     ]
    }
   ],
   "source": [
    "vectorizer_ngram_final = TfidfVectorizer(lowercase=True, ngram_range=(1, 2), max_features=11000)\n",
    "vectorizer_ngram_final.fit(X)\n",
    "X_ngram_tfidf_final = vectorizer_ngram_final.transform(X)\n",
    "\n",
    "lr_model_final = LogisticRegression(C=12)\n",
    "\n",
    "scores = cross_validate(lr_model_final, X_ngram_tfidf_final, y, scoring=scoring, cv=5)\n",
    "print_scores(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most informative features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coefficient</th>\n",
       "      <th>word</th>\n",
       "      <th>coefficient</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-6.953901</td>\n",
       "      <td>not</td>\n",
       "      <td>6.973223</td>\n",
       "      <td>great</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-4.587459</td>\n",
       "      <td>return</td>\n",
       "      <td>5.042313</td>\n",
       "      <td>excellent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-4.380018</td>\n",
       "      <td>poor</td>\n",
       "      <td>4.608211</td>\n",
       "      <td>price</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-4.143230</td>\n",
       "      <td>back</td>\n",
       "      <td>4.543380</td>\n",
       "      <td>perfect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-3.737640</td>\n",
       "      <td>terrible</td>\n",
       "      <td>3.904549</td>\n",
       "      <td>the best</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-3.681787</td>\n",
       "      <td>not work</td>\n",
       "      <td>3.894718</td>\n",
       "      <td>highly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-3.648012</td>\n",
       "      <td>after</td>\n",
       "      <td>3.722976</td>\n",
       "      <td>best</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-3.627613</td>\n",
       "      <td>returned</td>\n",
       "      <td>3.719372</td>\n",
       "      <td>memory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-3.458124</td>\n",
       "      <td>bad</td>\n",
       "      <td>3.536671</td>\n",
       "      <td>as</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-3.404955</td>\n",
       "      <td>waste</td>\n",
       "      <td>3.480402</td>\n",
       "      <td>for</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-3.350421</td>\n",
       "      <td>this thing</td>\n",
       "      <td>3.440436</td>\n",
       "      <td>works</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-3.206285</td>\n",
       "      <td>item</td>\n",
       "      <td>3.404274</td>\n",
       "      <td>fast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-3.186903</td>\n",
       "      <td>work</td>\n",
       "      <td>3.326391</td>\n",
       "      <td>used</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-3.178676</td>\n",
       "      <td>disappointed</td>\n",
       "      <td>3.293742</td>\n",
       "      <td>easy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-3.169460</td>\n",
       "      <td>worst</td>\n",
       "      <td>3.256047</td>\n",
       "      <td>no problems</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-3.117256</td>\n",
       "      <td>returning</td>\n",
       "      <td>3.230378</td>\n",
       "      <td>for the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-3.019373</td>\n",
       "      <td>get</td>\n",
       "      <td>3.127649</td>\n",
       "      <td>are</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-2.984875</td>\n",
       "      <td>money</td>\n",
       "      <td>3.117887</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-2.925129</td>\n",
       "      <td>unfortunately</td>\n",
       "      <td>3.087184</td>\n",
       "      <td>bit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-2.909681</td>\n",
       "      <td>maybe</td>\n",
       "      <td>3.077054</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    coefficient           word  coefficient         word\n",
       "0     -6.953901            not     6.973223        great\n",
       "1     -4.587459         return     5.042313    excellent\n",
       "2     -4.380018           poor     4.608211        price\n",
       "3     -4.143230           back     4.543380      perfect\n",
       "4     -3.737640       terrible     3.904549     the best\n",
       "5     -3.681787       not work     3.894718       highly\n",
       "6     -3.648012          after     3.722976         best\n",
       "7     -3.627613       returned     3.719372       memory\n",
       "8     -3.458124            bad     3.536671           as\n",
       "9     -3.404955          waste     3.480402          for\n",
       "10    -3.350421     this thing     3.440436        works\n",
       "11    -3.206285           item     3.404274         fast\n",
       "12    -3.186903           work     3.326391         used\n",
       "13    -3.178676   disappointed     3.293742         easy\n",
       "14    -3.169460          worst     3.256047  no problems\n",
       "15    -3.117256      returning     3.230378      for the\n",
       "16    -3.019373            get     3.127649          are\n",
       "17    -2.984875          money     3.117887         love\n",
       "18    -2.925129  unfortunately     3.087184          bit\n",
       "19    -2.909681          maybe     3.077054         good"
      ]
     },
     "execution_count": 1197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_model = LogisticRegression(C=12)\n",
    "lr_model.fit(X_ngram_tfidf_final, y)\n",
    "\n",
    "feature_names = vectorizer_ngram_final.get_feature_names() \n",
    "coefs_with_fns = sorted(zip(lr_model.coef_[0], feature_names)) \n",
    "coef_word=pd.DataFrame(coefs_with_fns)\n",
    "coef_word.columns='coefficient','word'\n",
    "most_pos = coef_word.sort_values(by='coefficient', ascending=True).head(20).reset_index(drop=True)\n",
    "most_neg = coef_word.sort_values(by='coefficient', ascending=False).head(20).reset_index(drop=True)\n",
    "pd.concat([most_pos, most_neg], axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
